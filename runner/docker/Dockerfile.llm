# Based on https://github.com/huggingface/api-inference-community/blob/main/docker_images/diffusers/Dockerfile

FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu20.04
LABEL maintainer="Yondon Fu <yondon@livepeer.org>"

ENV DEBIAN_FRONTEND=noninteractive

# Install prerequisites
RUN apt-get update && \
    apt-get install -y build-essential libssl-dev zlib1g-dev libbz2-dev \
    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \
    xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git \
    ffmpeg

# Install uv (pinned version copied from official image)
COPY --from=ghcr.io/astral-sh/uv:0.9.17 /uv /uvx /bin/

# Install Python 3.11 via uv and create venv
RUN uv python install 3.11 && uv venv
ENV PATH="/app/.venv/bin:$PATH"

# Upgrade pip and install your desired packages
ARG PIP_VERSION=24.2
RUN uv pip install --no-cache --upgrade pip==${PIP_VERSION} setuptools==69.5.1 wheel==0.43.0 && \
    uv pip install --no-cache torch==2.4.0 torchvision torchaudio

ARG VERSION="undefined"
ENV VERSION=${VERSION}

WORKDIR /app

# Copy lockfile and pyproject.toml first (for layer caching)
COPY pyproject.toml uv.lock ./

# Install dependencies
RUN uv sync --locked --extra llm --no-install-project

# Most DL models are quite large in terms of memory, using workers is a HUGE
# slowdown because of the fork and GIL with python.
# Using multiple pods seems like a better default strategy.
# Feel free to override if it does not make sense for your library.
ARG max_workers=1
ENV MAX_WORKERS=$max_workers
ENV HUGGINGFACE_HUB_CACHE=/models
ENV DIFFUSERS_CACHE=/models
ENV MODEL_DIR=/models
# This ensures compatbility with how GPUs are addressed within go-livepeer
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID

# vLLM configuration
ENV USE_8BIT=false
ENV MAX_NUM_BATCHED_TOKENS=8192
ENV MAX_NUM_SEQS=128
ENV MAX_MODEL_LEN=8192
ENV GPU_MEMORY_UTILIZATION=0.85
ENV TENSOR_PARALLEL_SIZE=1
ENV PIPELINE_PARALLEL_SIZE=1
# To use multiple GPUs, set TENSOR_PARALLEL_SIZE and PIPELINE_PARALLEL_SIZE
# Total GPUs used = TENSOR_PARALLEL_SIZE Ã— PIPELINE_PARALLEL_SIZE
# Example for 4 GPUs:
# - Option 1: TENSOR_PARALLEL_SIZE=2, PIPELINE_PARALLEL_SIZE=2
# - Option 2: TENSOR_PARALLEL_SIZE=4, PIPELINE_PARALLEL_SIZE=1
# - Option 3: TENSOR_PARALLEL_SIZE=1, PIPELINE_PARALLEL_SIZE=4

# Copy application files
COPY src/runner/ /app/src/runner
COPY images/ /app/images
COPY bench.py /app/bench.py
COPY example_data/ /app/example_data

# Final sync to install the project
RUN uv sync --locked --extra llm

CMD ["uv", "run", "--frozen", "uvicorn", "runner.main:app", "--log-config", "src/runner/cfg/uvicorn_logging_config.json", "--host", "", "--port", "8000"]
